{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d022fb-bee1-4faf-80cd-e0b1cc7d39c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all tables from the schema\n",
    "tables = spark.catalog.listTables(\"eldenringcatalog.gold\")\n",
    "\n",
    "for t in tables:\n",
    "    table_name = f\"eldenringcatalog.gold.{t.name}\"\n",
    "    print(f\"Dropping table: {table_name}\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fc93f6a-67c7-411f-871e-0cbe4aed4386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, when, coalesce, explode, split,\n",
    "    from_json, regexp_extract, regexp_replace, trim, upper, lower,\n",
    "    udf, struct, array, size, concat_ws, substring, expr,\n",
    "    sum as spark_sum, count as spark_count, avg, max, min,\n",
    "    monotonically_increasing_id, row_number, rank, dense_rank,\n",
    "    round as spark_round, abs as spark_abs, sqrt, pow,\n",
    "    first, last, collect_list, collect_set,\n",
    "    year, month, dayofmonth, date_format, to_date, datediff\n",
    ")\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, BooleanType, ArrayType, MapType, DateType\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333b51fd-ee99-46f4-b73c-a666db5e22a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **DIMENSION TABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfa4f8a3-6420-4987-86d4-761a7c23e823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_dim_weapons(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create weapon dimension with base attributes\n",
    "    \n",
    "    Source: silver.weapons\n",
    "    Grain: One row per weapon\n",
    "    Type: SCD Type 1\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating dim_weapons...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.weapons\")\n",
    "    \n",
    "    df_dim = df.select(\n",
    "        monotonically_increasing_id().alias(\"weapon_key\"),\n",
    "        col(\"weapon_id\"),\n",
    "        col(\"weapon_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"damage_type\"),\n",
    "        col(\"weight\"),\n",
    "        col(\"is_dlc\"),\n",
    "        \n",
    "        # Requirements (already parsed in Silver)\n",
    "        col(\"required_str\").cast(\"int\").alias(\"required_strength\"),\n",
    "        col(\"required_dex\").cast(\"int\").alias(\"required_dexterity\"),\n",
    "        col(\"required_int\").cast(\"int\").alias(\"required_intelligence\"),\n",
    "        col(\"required_fai\").cast(\"int\").alias(\"required_faith\"),\n",
    "        col(\"required_arc\").cast(\"int\").alias(\"required_arcane\"),\n",
    "        \n",
    "        # Passive effect\n",
    "        col(\"passive_effect\"),\n",
    "        \n",
    "        # Metadata\n",
    "        current_timestamp().alias(\"created_at\"),\n",
    "        current_timestamp().alias(\"updated_at\")\n",
    "    )\n",
    "    \n",
    "    return df_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a687633-90b2-4861-b27d-446750f7d926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_dim_shields(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create shield dimension\n",
    "    \n",
    "    Source: silver.shields\n",
    "    Grain: One row per shield\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating dim_shields...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.shields\")\n",
    "    \n",
    "    df_dim = df.select(\n",
    "        monotonically_increasing_id().alias(\"shield_key\"),\n",
    "        col(\"shield_id\"),\n",
    "        col(\"shield_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"weight\"),\n",
    "        col(\"is_dlc\"),\n",
    "        \n",
    "        # Requirements\n",
    "        col(\"required_str\").cast(\"int\").alias(\"required_strength\"),\n",
    "        col(\"required_dex\").cast(\"int\").alias(\"required_dexterity\"),\n",
    "        col(\"required_int\").cast(\"int\").alias(\"required_intelligence\"),\n",
    "        col(\"required_fai\").cast(\"int\").alias(\"required_faith\"),\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\"),\n",
    "        current_timestamp().alias(\"updated_at\")\n",
    "    )\n",
    "    \n",
    "    return df_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f0a4f9e-2d57-4cc2-9f1c-cdb7229d3099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_dim_armors(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create armor dimension\n",
    "    \n",
    "    Source: silver.armors\n",
    "    Grain: One row per armor piece\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating dim_armors...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.armors\")\n",
    "    \n",
    "    df_dim = df.select(\n",
    "        monotonically_increasing_id().alias(\"armor_key\"),\n",
    "        col(\"armor_id\"),\n",
    "        col(\"armor_name\"),\n",
    "        col(\"category\").alias(\"armor_type\"),  # Head, Chest, Gauntlets, Legs\n",
    "        col(\"weight\"),\n",
    "        col(\"is_dlc\"),\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\"),\n",
    "        current_timestamp().alias(\"updated_at\")\n",
    "    )\n",
    "    \n",
    "    return df_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5ed654-0cbb-4ad5-80b4-e3470bd27350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_dim_items(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create unified item dimension\n",
    "    \n",
    "    Source: silver.items_unified (consolidated from 13 item types)\n",
    "    Grain: One row per item\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating dim_items...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.items_unified\")\n",
    "    \n",
    "    df_dim = df.select(\n",
    "        monotonically_increasing_id().alias(\"item_key\"),\n",
    "        col(\"item_id\") ,\n",
    "        col(\"item_name\"),\n",
    "        col(\"item_type\"),  # ammo, bell, consumable, cookbook, etc.\n",
    "        col(\"description\"),\n",
    "        col(\"effect\"),\n",
    "        \n",
    "        # Type-specific attributes\n",
    "               \n",
    "        current_timestamp().alias(\"created_at\"),\n",
    "        current_timestamp().alias(\"updated_at\")\n",
    "    )\n",
    "    \n",
    "    return df_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0e111d-9b2a-4d04-85fe-e11e6516311d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_dim_locations(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create location dimension\n",
    "    \n",
    "    Source: silver.locations\n",
    "    Grain: One row per location\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating dim_locations...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.locations\")\n",
    "    \n",
    "    df_dim = df.select(\n",
    "        monotonically_increasing_id().alias(\"location_key\"),\n",
    "        col(\"location_id\"),\n",
    "        col(\"location_name\"),\n",
    "        col(\"region\"),\n",
    "        col(\"description\"),\n",
    "        col(\"is_dlc\"),\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\"),\n",
    "        current_timestamp().alias(\"updated_at\")\n",
    "    )\n",
    "    \n",
    "    return df_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ad0a67-8374-4dfb-b225-fe596bcc268d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_dim_npcs(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create NPC dimension\n",
    "    \n",
    "    Source: silver.npcs\n",
    "    Grain: One row per NPC\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating dim_npcs...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.npcs\")\n",
    "    \n",
    "    df_dim = df.select(\n",
    "        monotonically_increasing_id().alias(\"npc_key\"),\n",
    "        col(\"npc_id\"),\n",
    "        col(\"npc_name\"),\n",
    "        col(\"role\"),\n",
    "        col(\"voiced_by\").alias(\"voice_actor\"),\n",
    "        col(\"is_dlc\"),\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\"),\n",
    "        current_timestamp().alias(\"updated_at\")\n",
    "    )\n",
    "    \n",
    "    return df_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e136c1c-8818-4e4e-bdf1-1cf377834884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_dim_bosses(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create boss dimension\n",
    "    \n",
    "    Source: silver.bosses\n",
    "    Grain: One row per boss\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating dim_bosses...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.bosses\")\n",
    "    \n",
    "    df_dim = df.select(\n",
    "        monotonically_increasing_id().alias(\"boss_key\"),\n",
    "        col(\"boss_id\"),\n",
    "        col(\"boss_name\"),\n",
    "        col(\"is_dlc\"),\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\"),\n",
    "        current_timestamp().alias(\"updated_at\")\n",
    "    )\n",
    "    \n",
    "    return df_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee4aa28-8877-4173-a5a3-02ad31722af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_dim_date(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create date dimension for time-series analysis\n",
    "    \n",
    "    Grain: One row per date\n",
    "    Range: 2022-02-25 (Elden Ring release) to 2026-12-31\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating dim_date...\")\n",
    "    \n",
    "    # Generate date range\n",
    "    start_date = datetime(2022, 2, 25)  # Elden Ring release date\n",
    "    end_date = datetime(2026, 12, 31)\n",
    "    \n",
    "    dates = []\n",
    "    current = start_date\n",
    "    while current <= end_date:\n",
    "        dates.append((current,))\n",
    "        current += timedelta(days=1)\n",
    "    \n",
    "    df_dates = spark.createDataFrame(dates, [\"date\"])\n",
    "    \n",
    "    df_dim = df_dates.select(\n",
    "        to_date(col(\"date\")).alias(\"date_key\"),\n",
    "        year(col(\"date\")).alias(\"year\"),\n",
    "        month(col(\"date\")).alias(\"month\"),\n",
    "        dayofmonth(col(\"date\")).alias(\"day\"),\n",
    "        date_format(col(\"date\"), \"E\").alias(\"day_of_week\"),\n",
    "        date_format(col(\"date\"), \"MMMM\").alias(\"month_name\"),\n",
    "        date_format(col(\"date\"), \"Q\").cast(\"int\").alias(\"quarter\"),\n",
    "        when(month(col(\"date\")).isin([12, 1, 2]), \"Winter\")\n",
    "            .when(month(col(\"date\")).isin([3, 4, 5]), \"Spring\")\n",
    "            .when(month(col(\"date\")).isin([6, 7, 8]), \"Summer\")\n",
    "            .otherwise(\"Fall\").alias(\"season\"),\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "    \n",
    "    return df_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32093c5c-0e8e-45a0-838e-3be7c6d5d4bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_fact_weapon_stats(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create weapon performance fact table\n",
    "    \n",
    "    Source: silver.weapons_upgrades (60K rows)\n",
    "    Grain: One row per weapon per upgrade level\n",
    "    Metrics: Attack power, scaling, status buildup\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCC8 Creating fact_weapon_stats...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.weapons_upgrades\")\n",
    "    \n",
    "    # Get weapon dimension for lookup\n",
    "    df_weapons = spark.table(f\"{config['catalog']}.{config['silver_schema']}.weapons\") \\\n",
    "        .select(\n",
    "            col(\"weapon_name\"),\n",
    "            col(\"category\"),\n",
    "            col(\"weight\"),\n",
    "            col(\"required_str\"),\n",
    "            col(\"required_dex\"),\n",
    "            col(\"required_int\"),\n",
    "            col(\"required_fai\"),\n",
    "            col(\"required_arc\")\n",
    "        )\n",
    "    \n",
    "    # Join with explicit column references\n",
    "    df_joined = df.join(\n",
    "        df_weapons,\n",
    "        df[\"weapon_name\"] == df_weapons[\"weapon_name\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # FIX: Include weight and requirements in the select before using them in withColumn\n",
    "    df_fact = df_joined.select(\n",
    "        monotonically_increasing_id().alias(\"weapon_stat_key\"),\n",
    "        \n",
    "        # Dimensions (use df prefix to avoid ambiguity)\n",
    "        df[\"weapon_name\"],\n",
    "        df_weapons[\"category\"],\n",
    "        col(\"upgrade_level\"),\n",
    "        \n",
    "        # Attack stats (parsed in Silver)\n",
    "        col(\"attack_physical\").cast(\"double\"),\n",
    "        col(\"attack_magic\").cast(\"double\"),\n",
    "        col(\"attack_fire\").cast(\"double\"),\n",
    "        col(\"attack_lightning\").cast(\"double\"),\n",
    "        col(\"attack_holy\").cast(\"double\"),\n",
    "        col(\"stamina_cost\").cast(\"double\"),\n",
    "        col(\"critical_damage\").cast(\"double\"),\n",
    "        \n",
    "        # Total attack power\n",
    "        (\n",
    "            coalesce(col(\"attack_physical\"), lit(0.0)) + \n",
    "            coalesce(col(\"attack_magic\"), lit(0.0)) + \n",
    "            coalesce(col(\"attack_fire\"), lit(0.0)) + \n",
    "            coalesce(col(\"attack_lightning\"), lit(0.0)) + \n",
    "            coalesce(col(\"attack_holy\"), lit(0.0))\n",
    "        ).alias(\"total_attack_power\"),\n",
    "        \n",
    "        # Scaling stats\n",
    "        col(\"scaling_str\").alias(\"scaling_strength\"),\n",
    "        col(\"scaling_dex\").alias(\"scaling_dexterity\"),\n",
    "        col(\"scaling_int\").alias(\"scaling_intelligence\"),\n",
    "        col(\"scaling_fai\").alias(\"scaling_faith\"),\n",
    "        col(\"scaling_arc\").alias(\"scaling_arcane\"),\n",
    "        \n",
    "        # Guard stats (for weapons with shields)\n",
    "        col(\"guard_physical\").cast(\"double\"),\n",
    "        col(\"guard_magic\").cast(\"double\"),\n",
    "        col(\"guard_fire\").cast(\"double\"),\n",
    "        col(\"guard_lightning\").cast(\"double\"),\n",
    "        col(\"guard_holy\").cast(\"double\"),\n",
    "        col(\"guard_boost\").cast(\"double\"),\n",
    "        col(\"guard_resistance\").cast(\"double\"),\n",
    "        \n",
    "        # Passive effects (dual buildup support)\n",
    "        col(\"passive_effect_type\"),\n",
    "        col(\"passive_buildup_primary\").cast(\"int\"),\n",
    "        col(\"passive_buildup_secondary\").cast(\"int\"),\n",
    "        col(\"has_dual_buildup\").cast(\"boolean\"),\n",
    "        col(\"has_passive_effect\").cast(\"boolean\"),\n",
    "        \n",
    "        # FIX: Include these columns from df_weapons so we can use them in withColumn\n",
    "        df_weapons[\"weight\"],\n",
    "        df_weapons[\"required_str\"],\n",
    "        df_weapons[\"required_dex\"],\n",
    "        df_weapons[\"required_int\"],\n",
    "        df_weapons[\"required_fai\"],\n",
    "        df_weapons[\"required_arc\"],\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    ).withColumn(\n",
    "        \"damage_per_stat_point\",\n",
    "        when(\n",
    "            (coalesce(col(\"required_str\"), lit(0)) + \n",
    "             coalesce(col(\"required_dex\"), lit(0))) > 0,\n",
    "            coalesce(col(\"attack_physical\"), lit(0.0)) / \n",
    "            (coalesce(col(\"required_str\"), lit(0)) + \n",
    "             coalesce(col(\"required_dex\"), lit(0)))\n",
    "        ).otherwise(lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"damage_per_weight\",\n",
    "        when(\n",
    "            coalesce(col(\"weight\"), lit(0.0)) > 0,\n",
    "            coalesce(col(\"attack_physical\"), lit(0.0)) / \n",
    "            coalesce(col(\"weight\"), lit(1.0))\n",
    "        ).otherwise(lit(0.0))\n",
    "    ).drop(\n",
    "        # Drop requirement columns after calculation (optional - keep if you want them in the fact table)\n",
    "        \"required_str\", \"required_dex\", \"required_int\", \"required_fai\", \"required_arc\"\n",
    "    )\n",
    "    \n",
    "    return df_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61d9378-2af6-4bdf-90c3-d0f4d45549e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_fact_shield_stats(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create shield effectiveness fact table\n",
    "    \n",
    "    Source: silver.shields_upgrades (28K rows)\n",
    "    Grain: One row per shield per upgrade level\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCC8 Creating fact_shield_stats...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.shields_upgrades\")\n",
    "    \n",
    "    df_shields = spark.table(f\"{config['catalog']}.{config['silver_schema']}.shields\") \\\n",
    "        .select(\n",
    "            col(\"shield_name\").alias(\"base_shield_name\"),\n",
    "            col(\"weight\")\n",
    "        )\n",
    "    \n",
    "    # FIX: Use different column names to avoid ambiguity\n",
    "    df_joined = df.join(\n",
    "        df_shields,\n",
    "        df[\"shield_name\"] == df_shields[\"base_shield_name\"],\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    df_fact = df_joined.select(\n",
    "        monotonically_increasing_id().alias(\"shield_stat_key\"),\n",
    "        \n",
    "        # Use df prefix for shield_name from shields_upgrades table\n",
    "        df[\"shield_name\"],\n",
    "        col(\"upgrade_level\"),\n",
    "        \n",
    "        # Attack stats (shield bash damage)\n",
    "        col(\"attack_physical\").cast(\"double\"),\n",
    "        col(\"stamina_cost\").cast(\"double\"),\n",
    "        \n",
    "        # Guard stats (damage negation)\n",
    "        col(\"guard_physical\").cast(\"double\").alias(\"negation_physical\"),\n",
    "        col(\"guard_magic\").cast(\"double\").alias(\"negation_magic\"),\n",
    "        col(\"guard_fire\").cast(\"double\").alias(\"negation_fire\"),\n",
    "        col(\"guard_lightning\").cast(\"double\").alias(\"negation_lightning\"),\n",
    "        col(\"guard_holy\").cast(\"double\").alias(\"negation_holy\"),\n",
    "        col(\"guard_boost\").cast(\"double\").alias(\"boost\"),\n",
    "        col(\"guard_resistance\").cast(\"double\"),\n",
    "        \n",
    "        # Scaling\n",
    "        col(\"scaling_str\").alias(\"scaling_strength\"),\n",
    "        col(\"scaling_dex\").alias(\"scaling_dexterity\"),\n",
    "        \n",
    "        # Passive effects\n",
    "        col(\"passive_effect_type\"),\n",
    "        col(\"passive_buildup_primary\").cast(\"int\"),\n",
    "        col(\"passive_buildup_secondary\").cast(\"int\"),\n",
    "        col(\"has_dual_buildup\").cast(\"boolean\"),\n",
    "        col(\"has_passive_effect\").cast(\"boolean\"),\n",
    "        \n",
    "        # FIX: Include weight from base shields table in select\n",
    "        df_shields[\"weight\"],\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    ).withColumn(\n",
    "        # Total negation\n",
    "        \"total_negation\",\n",
    "        (\n",
    "            coalesce(col(\"negation_physical\"), lit(0.0)) + \n",
    "            coalesce(col(\"negation_magic\"), lit(0.0)) + \n",
    "            coalesce(col(\"negation_fire\"), lit(0.0)) + \n",
    "            coalesce(col(\"negation_lightning\"), lit(0.0)) + \n",
    "            coalesce(col(\"negation_holy\"), lit(0.0))\n",
    "        )\n",
    "    ).withColumn(\n",
    "        # Protection per weight - safe division\n",
    "        \"protection_per_weight\",\n",
    "        when(\n",
    "            coalesce(col(\"weight\"), lit(0.0)) > 0,\n",
    "            coalesce(col(\"negation_physical\"), lit(0.0)) / \n",
    "            coalesce(col(\"weight\"), lit(1.0))\n",
    "        ).otherwise(lit(0.0))\n",
    "    )\n",
    "    \n",
    "    return df_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57bb75c5-bb58-4b1b-ad91-c1cc0d24a6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_fact_armor_stats(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create armor protection fact table\n",
    "    \n",
    "    Source: silver.armors (723 rows)\n",
    "    Grain: One row per armor piece\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCC8 Creating fact_armor_stats...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.armors\")\n",
    "    \n",
    "    df_fact = df.select(\n",
    "        monotonically_increasing_id().alias(\"armor_stat_key\"),\n",
    "        \n",
    "        col(\"armor_name\"),\n",
    "        col(\"category\").alias(\"armor_type\"),\n",
    "        col(\"weight\"),\n",
    "        \n",
    "        # Damage negation (parsed in Silver)\n",
    "        col(\"dmg_negation_physical\").cast(\"double\").alias(\"negation_physical\"),\n",
    "        col(\"dmg_negation_vs_strike\").cast(\"double\").alias(\"negation_strike\"),\n",
    "        col(\"dmg_negation_vs_slash\").cast(\"double\").alias(\"negation_slash\"),\n",
    "        col(\"dmg_negation_vs_pierce\").cast(\"double\").alias(\"negation_pierce\"),\n",
    "        col(\"dmg_negation_magic\").cast(\"double\").alias(\"negation_magic\"),\n",
    "        col(\"dmg_negation_fire\").cast(\"double\").alias(\"negation_fire\"),\n",
    "        col(\"dmg_negation_lightning\").cast(\"double\").alias(\"negation_lightning\"),\n",
    "        col(\"dmg_negation_holy\").cast(\"double\").alias(\"negation_holy\"),\n",
    "        \n",
    "        # Resistance stats\n",
    "        col(\"resistance_immunity\").cast(\"double\").alias(\"resistance_immunity\"),\n",
    "        col(\"resistance_robustness\").cast(\"double\").alias(\"resistance_robustness\"),\n",
    "        col(\"resistance_focus\").cast(\"double\").alias(\"resistance_focus\"),\n",
    "        col(\"resistance_vitality\").cast(\"double\").alias(\"resistance_vitality\"),\n",
    "        col(\"resistance_poise\").cast(\"double\").alias(\"resistance_poise\"),\n",
    "        \n",
    "        # Calculated metrics\n",
    "        (coalesce(col(\"dmg_negation_physical\"), lit(0)) / \n",
    "         coalesce(col(\"weight\"), lit(1))).alias(\"protection_per_weight\"),\n",
    "        \n",
    "        (coalesce(col(\"resistance_poise\"), lit(0)) / \n",
    "         coalesce(col(\"weight\"), lit(1))).alias(\"poise_per_weight\"),\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "    \n",
    "    return df_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "115fd676-c2b4-44ec-ad45-3ffec5158d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_fact_boss_encounters(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create boss encounter fact table\n",
    "    \n",
    "    Source: silver.bosses (155 rows)\n",
    "    Grain: One row per boss\n",
    "    Includes: HP ranges, phases, classifications\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCC8 Creating fact_boss_encounters...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.bosses\")\n",
    "    \n",
    "    df_fact = df.select(\n",
    "        monotonically_increasing_id().alias(\"boss_encounter_key\"),\n",
    "        \n",
    "        col(\"boss_name\"),\n",
    "        # HP metrics (parsed in Silver with parse_boss_hp UDF)\n",
    "        col(\"hp_min\").cast(\"int\"),\n",
    "        col(\"hp_max\").cast(\"int\"),\n",
    "        col(\"hp_type\"),  # single, phased, approximate, tbd\n",
    "        col(\"phase_count\").cast(\"int\"),\n",
    "        col(\"boss_classification\"),  # Single-Phase, Multi-Phase, God-Tier, etc.\n",
    "        \n",
    "        # Calculated difficulty score (simplified)\n",
    "        when(col(\"boss_classification\") == \"God-Tier\", lit(100))\n",
    "            .when(col(\"boss_classification\") == \"Multi-Phase\", lit(80))\n",
    "            .when(col(\"boss_classification\") == \"High-HP\", lit(60))\n",
    "            .when(col(\"boss_classification\") == \"Standard\", lit(40))\n",
    "            .otherwise(lit(20)).alias(\"difficulty_score\"),\n",
    "        \n",
    "        col(\"is_dlc\").cast(\"int\"),\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "    \n",
    "    return df_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6fcec0e-cf48-478b-a5c6-bfa4a35e64f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_fact_spell_usage(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create spell usage fact table\n",
    "    \n",
    "    Sources: silver.sorceries (86), silver.incantations (131)\n",
    "    Grain: One row per spell\n",
    "    Metrics: FP costs (min/max/per_use structure), requirements\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCC8 Creating fact_spell_usage...\")\n",
    "    \n",
    "    # Sorceries\n",
    "    df_sorc = spark.table(\n",
    "        f\"{config['catalog']}.{config['silver_schema']}.sorceries\"\n",
    "    ).select(\n",
    "        lit(\"Sorcery\").alias(\"spell_type\"),\n",
    "        col(\"sorcery_name\").alias(\"spell_name\"),\n",
    "        col(\"required_int\").cast(\"int\").alias(\"required_intelligence\"),\n",
    "        col(\"required_fai\").cast(\"int\").alias(\"required_faith\"),\n",
    "        col(\"required_arc\").cast(\"int\").alias(\"required_arcane\"),\n",
    "        col(\"memory_slots\").cast(\"int\").alias(\"slots_required\"),\n",
    "        col(\"stamina_cost\").cast(\"double\"),\n",
    "        col(\"fp_cost_min\").cast(\"int\"),\n",
    "        col(\"fp_cost_max\").cast(\"int\"),\n",
    "        col(\"fp_cost_per_use\").cast(\"int\"),\n",
    "        col(\"fp_cost_type\"),\n",
    "        col(\"bonus\"),\n",
    "        col(\"is_dlc\").cast(\"int\")\n",
    "    )\n",
    "    \n",
    "    # Incantations\n",
    "    df_incan = spark.table(\n",
    "        f\"{config['catalog']}.{config['silver_schema']}.incantations\"\n",
    "    ).select(\n",
    "        lit(\"Incantation\").alias(\"spell_type\"),\n",
    "        col(\"incantation_name\").alias(\"spell_name\"),\n",
    "        col(\"required_int\").cast(\"int\").alias(\"required_intelligence\"),\n",
    "        col(\"required_fai\").cast(\"int\").alias(\"required_faith\"),\n",
    "        col(\"required_arc\").cast(\"int\").alias(\"required_arcane\"),\n",
    "        col(\"memory_slots\").cast(\"int\").alias(\"slots_required\"),\n",
    "        col(\"stamina_cost\").cast(\"double\"),\n",
    "        col(\"fp_cost_min\").cast(\"int\"),\n",
    "        col(\"fp_cost_max\").cast(\"int\"),\n",
    "        col(\"fp_cost_per_use\").cast(\"int\"),\n",
    "        col(\"fp_cost_type\"),\n",
    "        col(\"bonus\"),\n",
    "        col(\"is_dlc\").cast(\"int\").alias(\"is_dlc\")\n",
    "    )\n",
    "    \n",
    "    df_fact = df_sorc.union(df_incan).select(\n",
    "        monotonically_increasing_id().alias(\"spell_usage_key\"),\n",
    "        col(\"*\"),\n",
    "        ((coalesce(col(\"fp_cost_min\"), lit(0)) + coalesce(col(\"fp_cost_max\"), lit(0))) / 2.0).alias(\"fp_efficiency\"),\n",
    "        (coalesce(col(\"required_intelligence\"), lit(0)) +\n",
    "         coalesce(col(\"required_faith\"), lit(0)) +\n",
    "         coalesce(col(\"required_arcane\"), lit(0))).alias(\"total_stat_requirement\"),\n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "    \n",
    "    return df_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d7f4411-d230-491d-b4c1-771c36ff8126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_fact_status_effects(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create status effects fact table\n",
    "    \n",
    "    Source: silver.weapons_upgrades (passive effects with dual buildup)\n",
    "    Grain: One row per weapon per upgrade level with status effect\n",
    "    Focus: Buildup efficiency analysis\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCC8 Creating fact_status_effects...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.weapons_upgrades\")\n",
    "    \n",
    "    # Filter only weapons with passive effects\n",
    "    df_fact = df.filter(col(\"passive_effect_type\").isNotNull()).select(\n",
    "        monotonically_increasing_id().alias(\"status_effect_key\"),\n",
    "        \n",
    "        col(\"weapon_name\"),\n",
    "        col(\"upgrade_level\"),\n",
    "        \n",
    "        col(\"passive_effect_type\").alias(\"effect_type\"),  # poison, frostbite, bleed, etc.\n",
    "        col(\"passive_buildup_primary\").cast(\"int\").alias(\"buildup_primary\"),\n",
    "        col(\"passive_buildup_secondary\").cast(\"int\").alias(\"buildup_secondary\"),\n",
    "        col(\"has_dual_buildup\").cast(\"boolean\").alias(\"has_dual_buildup\"),\n",
    "        \n",
    "        # Total buildup\n",
    "        (coalesce(col(\"passive_buildup_primary\"), lit(0)) + \n",
    "         coalesce(col(\"passive_buildup_secondary\"), lit(0))).alias(\"total_buildup\"),\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "    \n",
    "    return df_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad55f7a5-61c7-4dee-855a-d612d2141352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_bridge_location_items(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Bridge: Locations ↔ Items\n",
    "    Source: silver.location_items (2,937 relationships)\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDD17 Creating bridge_location_items...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.location_items\")\n",
    "    \n",
    "    return df.select(\n",
    "        monotonically_increasing_id().alias(\"location_item_key\"),\n",
    "        col(\"location_id\"),\n",
    "        col(\"item_name\"),\n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "\n",
    "def create_bridge_location_npcs(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Bridge: Locations ↔ NPCs\n",
    "    Source: silver.location_npcs (215 relationships)\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDD17 Creating bridge_location_npcs...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.location_npcs\")\n",
    "    \n",
    "    return df.select(\n",
    "        monotonically_increasing_id().alias(\"location_npc_key\"),\n",
    "        col(\"location_id\"),\n",
    "        col(\"npc_name\"),\n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "\n",
    "def create_bridge_location_creatures(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Bridge: Locations ↔ Creatures\n",
    "    Source: silver.location_creatures (789 relationships)\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDD17 Creating bridge_location_creatures...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.location_creatures\")\n",
    "    \n",
    "    return df.select(\n",
    "        monotonically_increasing_id().alias(\"location_creature_key\"),\n",
    "        col(\"location_id\"),\n",
    "        col(\"creature_name\"),\n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "\n",
    "def create_bridge_location_bosses(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Bridge: Locations ↔ Bosses\n",
    "    Source: silver.location_bosses (235 relationships)\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDD17 Creating bridge_location_bosses...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.location_bosses\")\n",
    "    \n",
    "    return df.select(\n",
    "        monotonically_increasing_id().alias(\"location_boss_key\"),\n",
    "        col(\"location_id\"),\n",
    "        col(\"boss_name\"),\n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "\n",
    "\n",
    "def create_bridge_boss_drops(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Bridge: Bosses → Drop Items (with classification)\n",
    "    \n",
    "    Source: silver.boss_drops (with drop_type classification)\n",
    "    Grain: One row per boss per location per drop item\n",
    "    \n",
    "    Drop types: runes, remembrance, great_rune, weapon, armor, \n",
    "                material, talisman, ash_of_war, item\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDD17 Creating bridge_boss_drops...\")\n",
    "    \n",
    "    df = spark.table(f\"{config['catalog']}.{config['silver_schema']}.boss_drops\")\n",
    "    \n",
    "    return df.select(\n",
    "        monotonically_increasing_id().alias(\"boss_drop_key\"),\n",
    "        col(\"boss_id\"),\n",
    "        col(\"boss_name\"),\n",
    "        col(\"location_name\"),\n",
    "        col(\"drop_item\"),\n",
    "        col(\"drop_type\"),  # Critical: 9 drop type classifications\n",
    "        col(\"drop_order\").cast(\"int\"),\n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ff4247-e183-4475-a7c7-4e9867791e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_agg_weapon_rankings(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pre-aggregated weapon rankings by damage efficiency\n",
    "    \n",
    "    Metrics:\n",
    "    - Damage per stat point (attack / (STR + DEX))\n",
    "    - Damage per weight\n",
    "    - Best weapons by category\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating agg_weapon_rankings...\")\n",
    "    \n",
    "    df_fact = spark.table(f\"{config['catalog']}.{config['gold_schema']}.fact_weapon_stats\")\n",
    "    \n",
    "    # Get max upgrade level for each weapon\n",
    "    window_max = Window.partitionBy(\"weapon_name\").orderBy(col(\"upgrade_level\").desc())\n",
    "    \n",
    "    df_max_upgrade = df_fact.withColumn(\"rn\", row_number().over(window_max)) \\\n",
    "        .filter(col(\"rn\") == 1) \\\n",
    "        .drop(\"rn\")\n",
    "    \n",
    "    # Rank by damage efficiency\n",
    "    window_rank = Window.partitionBy(\"category\").orderBy(col(\"damage_per_stat_point\").desc())\n",
    "    \n",
    "    df_agg = df_max_upgrade.select(\n",
    "        col(\"weapon_name\"),\n",
    "        col(\"category\"),\n",
    "        col(\"upgrade_level\"),\n",
    "        col(\"total_attack_power\"),\n",
    "        col(\"damage_per_stat_point\"),\n",
    "        col(\"damage_per_weight\"),\n",
    "        rank().over(window_rank).alias(\"rank_in_category\"),\n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2067323-8f05-483a-8be7-d8b938e12166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_agg_armor_efficiency(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pre-aggregated armor efficiency metrics\n",
    "    \n",
    "    Metrics:\n",
    "    - Protection per weight\n",
    "    - Poise per weight\n",
    "    - Best armor pieces by type\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating agg_armor_efficiency...\")\n",
    "    \n",
    "    df_fact = spark.table(f\"{config['catalog']}.{config['gold_schema']}.fact_armor_stats\")\n",
    "    \n",
    "    window_rank = Window.partitionBy(\"armor_type\").orderBy(col(\"protection_per_weight\").desc())\n",
    "    \n",
    "    df_agg = df_fact.select(\n",
    "        col(\"armor_name\"),\n",
    "        col(\"armor_type\"),\n",
    "        col(\"weight\"),\n",
    "        col(\"negation_physical\"),\n",
    "        col(\"protection_per_weight\"),\n",
    "        col(\"poise_per_weight\"),\n",
    "        rank().over(window_rank).alias(\"rank_in_type\"),\n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8f7b30-d85e-4fa6-88c7-83703eddd68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_agg_status_buildup(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pre-aggregated status effect buildup analysis\n",
    "    \n",
    "    Focus: Best weapons for inflicting status effects\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating agg_status_buildup...\")\n",
    "    \n",
    "    df_fact = spark.table(f\"{config['catalog']}.{config['gold_schema']}.fact_status_effects\")\n",
    "    \n",
    "    # Get max upgrade for each weapon-effect combination\n",
    "    window_max = Window.partitionBy(\"weapon_name\", \"effect_type\") \\\n",
    "        .orderBy(col(\"upgrade_level\").desc())\n",
    "    \n",
    "    df_max = df_fact.withColumn(\"rn\", row_number().over(window_max)) \\\n",
    "        .filter(col(\"rn\") == 1) \\\n",
    "        .drop(\"rn\")\n",
    "    \n",
    "    # Rank by total buildup\n",
    "    window_rank = Window.partitionBy(\"effect_type\").orderBy(col(\"total_buildup\").desc())\n",
    "    \n",
    "    df_agg = df_max.select(\n",
    "        col(\"weapon_name\"),\n",
    "        col(\"effect_type\"),\n",
    "        col(\"upgrade_level\"),\n",
    "        col(\"buildup_primary\"),\n",
    "        col(\"buildup_secondary\"),\n",
    "        col(\"total_buildup\"),\n",
    "        col(\"has_dual_buildup\"),\n",
    "        rank().over(window_rank).alias(\"rank_in_effect\"),\n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2baaecb4-0f97-4422-98c4-02399e58c4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_agg_boss_difficulty(config: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Pre-aggregated boss difficulty metrics\n",
    "    \n",
    "    Combines: HP, phases, drop value to estimate difficulty\n",
    "    \"\"\"\n",
    "    logger.info(\"\uD83D\uDCCA Creating agg_boss_difficulty...\")\n",
    "    \n",
    "    df_bosses = spark.table(f\"{config['catalog']}.{config['gold_schema']}.fact_boss_encounters\")\n",
    "    df_drops = spark.table(f\"{config['catalog']}.{config['gold_schema']}.bridge_boss_drops\")\n",
    "    \n",
    "    # Count drops by type\n",
    "    df_drop_counts = df_drops.groupBy(\"boss_name\").agg(\n",
    "        spark_count(\"*\").alias(\"total_drops\"),\n",
    "        spark_count(when(col(\"drop_type\") == \"remembrance\", 1)).alias(\"remembrance_count\"),\n",
    "        spark_count(when(col(\"drop_type\") == \"great_rune\", 1)).alias(\"great_rune_count\")\n",
    "    )\n",
    "    \n",
    "    df_agg = df_bosses.join(df_drop_counts, \"boss_name\", \"left\").select(\n",
    "        col(\"boss_name\"),\n",
    "        col(\"hp_min\"),\n",
    "        col(\"hp_max\"),\n",
    "        col(\"phase_count\"),  # ✅ FIXED: Changed from hp_phases to phase_count\n",
    "        col(\"boss_classification\"),  # ✅ FIXED: Changed from hp_classification to boss_classification\n",
    "        col(\"difficulty_score\"),\n",
    "        coalesce(col(\"total_drops\"), lit(0)).alias(\"total_drops\"),\n",
    "        coalesce(col(\"remembrance_count\"), lit(0)).alias(\"remembrance_count\"),\n",
    "        col(\"is_dlc\"),\n",
    "        \n",
    "        # Rank by difficulty\n",
    "        rank().over(Window.orderBy(col(\"difficulty_score\").desc())).alias(\"difficulty_rank\"),\n",
    "        \n",
    "        current_timestamp().alias(\"created_at\")\n",
    "    )\n",
    "    \n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc6ca8b-e326-497b-a0bb-e7a9b2f14ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_gold_transformations(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Execute all Gold layer transformations\n",
    "    \n",
    "    Creates:\n",
    "    - 8 dimension tables\n",
    "    - 6 fact tables\n",
    "    - 5 bridge tables\n",
    "    - 4 aggregate tables\n",
    "    \n",
    "    Total: 23 Gold tables\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        \"start_time\": datetime.now(),\n",
    "        \"tables_created\": [],\n",
    "        \"tables_failed\": []\n",
    "    }\n",
    "    \n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(\"\uD83C\uDFC6 STARTING GOLD LAYER TRANSFORMATIONS\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DIMENSION TABLES (8)\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\n\uD83D\uDCCA CREATING DIMENSION TABLES...\")\n",
    "    \n",
    "    dimensions = [\n",
    "        (\"dim_weapons\", create_dim_weapons),\n",
    "        (\"dim_shields\", create_dim_shields),\n",
    "        (\"dim_armors\", create_dim_armors),\n",
    "        (\"dim_items\", create_dim_items),\n",
    "        (\"dim_locations\", create_dim_locations),\n",
    "        (\"dim_npcs\", create_dim_npcs),\n",
    "        (\"dim_bosses\", create_dim_bosses),\n",
    "        (\"dim_date\", create_dim_date)\n",
    "    ]\n",
    "    \n",
    "    for table_name, transform_func in dimensions:\n",
    "        try:\n",
    "            df = transform_func(config)\n",
    "            full_name = f\"{config['catalog']}.{config['gold_schema']}.{table_name}\"\n",
    "            \n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(full_name)\n",
    "            \n",
    "            count = df.count()\n",
    "            stats[\"tables_created\"].append(full_name)\n",
    "            logger.info(f\"  ✅ {table_name}: {count:,} rows\")\n",
    "        except Exception as e:\n",
    "            stats[\"tables_failed\"].append(table_name)\n",
    "            logger.error(f\"  ❌ {table_name}: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FACT TABLES (6)\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\n\uD83D\uDCC8 CREATING FACT TABLES...\")\n",
    "    \n",
    "    facts = [\n",
    "        (\"fact_weapon_stats\", create_fact_weapon_stats),\n",
    "        (\"fact_shield_stats\", create_fact_shield_stats),\n",
    "        (\"fact_armor_stats\", create_fact_armor_stats),\n",
    "        (\"fact_boss_encounters\", create_fact_boss_encounters),\n",
    "        (\"fact_spell_usage\", create_fact_spell_usage),\n",
    "        (\"fact_status_effects\", create_fact_status_effects)\n",
    "    ]\n",
    "    \n",
    "    for table_name, transform_func in facts:\n",
    "        try:\n",
    "            df = transform_func(config)\n",
    "            full_name = f\"{config['catalog']}.{config['gold_schema']}.{table_name}\"\n",
    "            \n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(full_name)\n",
    "            \n",
    "            count = df.count()\n",
    "            stats[\"tables_created\"].append(full_name)\n",
    "            logger.info(f\"  ✅ {table_name}: {count:,} rows\")\n",
    "        except Exception as e:\n",
    "            stats[\"tables_failed\"].append(table_name)\n",
    "            logger.error(f\"  ❌ {table_name}: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # BRIDGE TABLES (5)\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\n\uD83D\uDD17 CREATING BRIDGE TABLES...\")\n",
    "    \n",
    "    bridges = [\n",
    "        (\"bridge_location_items\", create_bridge_location_items),\n",
    "        (\"bridge_location_npcs\", create_bridge_location_npcs),\n",
    "        (\"bridge_location_creatures\", create_bridge_location_creatures),\n",
    "        (\"bridge_location_bosses\", create_bridge_location_bosses),\n",
    "        (\"bridge_boss_drops\", create_bridge_boss_drops)\n",
    "    ]\n",
    "    \n",
    "    for table_name, transform_func in bridges:\n",
    "        try:\n",
    "            df = transform_func(config)\n",
    "            full_name = f\"{config['catalog']}.{config['gold_schema']}.{table_name}\"\n",
    "            \n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(full_name)\n",
    "            \n",
    "            count = df.count()\n",
    "            stats[\"tables_created\"].append(full_name)\n",
    "            logger.info(f\"  ✅ {table_name}: {count:,} rows\")\n",
    "        except Exception as e:\n",
    "            stats[\"tables_failed\"].append(table_name)\n",
    "            logger.error(f\"  ❌ {table_name}: {e}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # AGGREGATE TABLES (4) - Must run AFTER facts/bridges\n",
    "    # ========================================================================\n",
    "    logger.info(\"\\n\uD83D\uDCCA CREATING AGGREGATE TABLES...\")\n",
    "    \n",
    "    aggregates = [\n",
    "        (\"agg_weapon_rankings\", create_agg_weapon_rankings),\n",
    "        (\"agg_armor_efficiency\", create_agg_armor_efficiency),\n",
    "        (\"agg_status_buildup\", create_agg_status_buildup),\n",
    "        (\"agg_boss_difficulty\", create_agg_boss_difficulty)\n",
    "    ]\n",
    "    \n",
    "    for table_name, transform_func in aggregates:\n",
    "        try:\n",
    "            df = transform_func(config)\n",
    "            full_name = f\"{config['catalog']}.{config['gold_schema']}.{table_name}\"\n",
    "            \n",
    "            df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(full_name)\n",
    "            \n",
    "            count = df.count()\n",
    "            stats[\"tables_created\"].append(full_name)\n",
    "            logger.info(f\"  ✅ {table_name}: {count:,} rows\")\n",
    "        except Exception as e:\n",
    "            stats[\"tables_failed\"].append(table_name)\n",
    "            logger.error(f\"  ❌ {table_name}: {e}\")\n",
    "    \n",
    "    stats[\"end_time\"] = datetime.now()\n",
    "    stats[\"duration\"] = (stats[\"end_time\"] - stats[\"start_time\"]).total_seconds()\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\" * 80)\n",
    "    logger.info(\"\uD83C\uDFC6 GOLD LAYER COMPLETE\")\n",
    "    logger.info(f\"✅ Tables created: {len(stats['tables_created'])}\")\n",
    "    logger.info(f\"❌ Tables failed: {len(stats['tables_failed'])}\")\n",
    "    logger.info(f\"⏱️  Duration: {stats['duration']:.2f}s\")\n",
    "    logger.info(\"=\" * 80)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcfcba39-a56b-4bfe-945d-62558d51e781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"catalog\": \"eldenringcatalog\",\n",
    "        \"bronze_schema\": \"bronze\",\n",
    "        \"silver_schema\": \"silver\",\n",
    "        \"gold_schema\": \"gold\",\n",
    "        \"batch_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        stats = run_gold_transformations(CONFIG)\n",
    "        \n",
    "        logger.info(\"\\n\uD83D\uDCCA GOLD LAYER SUMMARY\")\n",
    "        logger.info(f\"Total tables: {len(stats['tables_created'])}\")\n",
    "        logger.info(\"\\n✅ CREATED TABLES:\")\n",
    "        for table in stats['tables_created']:\n",
    "            logger.info(f\"  • {table}\")\n",
    "        \n",
    "        if stats['tables_failed']:\n",
    "            logger.warning(\"\\n❌ FAILED TABLES:\")\n",
    "            for table in stats['tables_failed']:\n",
    "                logger.warning(f\"  • {table}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"\\n❌ Gold layer failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "177355ab-0ca8-4d31-a5d4-fa9dfefd9944",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764101135457}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from eldenringcatalog.gold.dim_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "277eaad5-0660-42d5-b288-5b9a6498737a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5100624023837561,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GOLD_LAYER",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}